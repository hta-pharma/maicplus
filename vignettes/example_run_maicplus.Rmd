---
title: "Example run maicplus"
output:
  pdf_document: default
  html_document: default
  word_document: default
---

```{r, echo= FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE, tidy = TRUE)
```

# Introduction

This package describes the steps required to perform a matching-adjusted indirect comparison (MAIC) analysis using the `maicplus` package in R where the endpoint of interest is either time-to-event (e.g. overall survival) or binary (e.g. objective tumor response).

The methods described in this document are based on those originally described by Signorovitch et al. 2010 and described in the National Institute for Health and Care Excellence (NICE) Decision Support Unit (DSU) Technical Support Document (TSD) 18. [signorovitch2010; phillippo2016a]

MAIC methods are often required when: 

* There is no common comparator treatment to link a clinical trial of a new intervention to clinical trials of other treatments in a given disease area. For example if the only study of a new intervention is a single arm trial with no control group. This is commonly referred to as an unanchored MAIC.
* A common comparator is available to link a clinical trial of a new intervention to a clinical trial of one other treatment in a given disease area but there are substantial differences in patient demographic or disease characteristics that are believed to be treatment effect modifiers. This is commonly referred to as an anchored MAIC.

The premise of MAIC methods is to adjust for between-trial differences in patient demographic or disease characteristics at baseline. When a common treatment comparator or ‘linked network’ are unavailable, a MAIC assumes that differences between absolute outcomes that would be observed in each trial are entirely explained by imbalances in prognostic variables and treatment effect
modifiers. Prognostic variables are those that are predictive of disease outcomes, independent of the treatment received. For example, older patients may have increased risk of death compared to younger patients. Treatment effect modifiers are those variables that influence the relative effect of one treatment compared to another. For example patients with a better performance status may experience a larger treatment benefit than those with a worse performance status. Under this assumption, every prognostic variable and every treatment effect modifier that is imbalanced between the two studies must be available. This assumption is generally considered very difficult to
meet. [phillippo2016a] There are several ways of identifying prognostic variables/treatment effect modifiers to be used in the MAIC analyses, some of which include:

* Clinical expertise (when available to a project)
* Published papers/previous submissions (what has been identified in the disease area previously)
* Univariable/multivariable regression analyses to identify which covariates have a significant effect on the outcome
* Subgroup analyses of clinical trials may identify interactions between patient characteristics and the relative treatment effect


# Theory behind MAIC

We will briefly go over the theory behind MAIC. For detailed information, see Signorovitch et al. 2010. 

Let us define $t_i$ to be the treatment patient $i$ received. We assume $t_i=0$ if the patient received intervention (IPD) and $t_i=1$ if the patient received comparator treatment. The causal effect of treatment $T=0$ vs $T=1$ on the mean of the outcome $Y$ can be estimated as below

\[
\frac{\sum_{i=1}^{n}y_{i}(1-t_{i})w_{i}}{\sum_{i=1}^{n}(1-t_{i})w_{i}}-\bar{y}_{1}
\]

where $w_i=\frac{Pr(T_i=1\mid x_i)}{Pr(T_i=0\mid x_i)}$ is the odds that patient $i$ received treatment $T=1$ vs $T=0$ (i.e. enrolls in aggregate data study vs IPD study) given baseline characteristics $x_i$. Thus, the patients receiving $T=0$ are re-weighted to match the distribution of patients receiving $T=1$. Note that this causal effect would be the case when the outcome $Y$ is continuous. If the outcome is binary, $Y$ would be a proportion and we would use a link function such as logit to give us the causal effect in an odds ratio scale. As in propensity score methods, we may assume $w_i$ to follow logistic regression form

\[
w_{i}=exp(x_i^{T}\beta)
\]

However, in order to estimate $\beta$, we cannot use maximum likelihood approach because we do not have IPD for both trials. Instead, we use method of moments. We estimate $\beta$ such that the weighted averages of the covariates in the IPD exactly matches the aggregate data averages. Mathematically speaking, we want to estimate $\beta$ such that:

\[
0=\frac{\sum_{i=1}^{n}x_{i}exp(x_i^{T}\hat{\beta})}{\sum_{i=1}^{n}exp(x_i^{T}\hat{\beta})}-\bar{x}_{agg}
\]

If the $x_i$ contains all confounders and the logistic regression for $w_i$ is correctly specified, we obtain a consistent estimate of the causal effect of intervention vs comparator treatment. Above equation is equivalent to

\[
0=\sum_{i=1}^{n}(x_{i}-\bar{x}_{agg})exp(x_{i}^{T}\hat{\beta})
\]

We could transform transform IPD by subtracting the aggregate data means (this is why centering is needed when preprocessing). 

\[
0=\sum_{i=1}^{n}x_{i}exp(x_{i}^{T}\hat{\beta})
\]

Note that this is the first derivative of

\[
Q(\beta)=\sum_{i=1}^{n}exp(x_{i}^{T}\hat{\beta})
\]

which has second derivative

\[
Q''(\beta)=\sum_{i=1}^{n}x_ix_i^Texp(x_{i}^{T}\hat{\beta})
\]

Since $Q''(\beta)$ is positive-definite for all $\beta$, $Q(\beta)$ is convex and any finite solution from the equation is unique and corresponds to the global minimum of $Q(\beta)$. Thus, we can use optimization methods to calculate $\beta$.

# Example scenario

We present an unanchored MAIC of two treatments in lung cancer. The two endpoints being compared are overall survival (a time to event outcome) and objective response (a binary outcome). The data available are:

* Individual patient data from a single arm study
* Aggregate summary data for the comparator study
* Psuedo patient data from the comparator study. This is not required for the matching process but is needed to derive the relative treatment effects between the intervention and comparator.

# Preprocessing

## Package load

```{r}
setwd("~/GitHub/maicplus")
devtools::load_all()

#devtools::install_github("hta-pharma/maicplus")
#library(maicplus)

library(dplyr) # this is used for data merging/cleaning. Package itself does not depend on dplyr

library(clubSandwich) # For robust standard error in logistic regression
library(sandwich)

library(survminer) # for ggsurvplot
library(ggplot2) # for ggplot functions
library(boot) # for bootstrapping
```

## Preprocessing IPD

In this example scenario, age, sex, the Eastern Cooperative Oncology Group (ECOG) performance status, smoking status, and number of previous treatments have been identified as imbalanced prognostic variables/treatment effect modifiers.

This example reads in and combines data from three standard simulated data sets (adsl, adrs and adtte) which are saved as '.csv' files.

```{r}
adsl <- read.csv(system.file("extdata", "adsl.csv", package = "maicplus",
                             mustWork = TRUE))
adrs <- read.csv(system.file("extdata", "adrs.csv", package = "maicplus",
                             mustWork = TRUE))
adtte <- read.csv(system.file("extdata", "adtte.csv", package = "maicplus", mustWork = TRUE))

# Data containing the matching variables
adsl <- adsl %>%
  mutate(SEX_MALE = ifelse(SEX=="Male", 1, 0)) %>%
  mutate(AGE_SQUARED = AGE^2)

# Could use built-in function for dummizing variables 
# adsl <- dummize_ipd(adsl, dummize_cols=c("SEX"), dummize_ref_level=c("Female"))
  
# Response data  
adrs <- adrs %>% 
  filter(PARAM == "Response") %>%
  transmute(USUBJID, ARM, RESPONSE=AVAL)

# Time to event data (overall survival)
adtte <- adtte %>% 
  filter(PARAMCD == "OS") %>%
  mutate(EVENT = 1-CNSR) %>% 
  transmute(USUBJID, ARM, TIME = AVAL, EVENT)

# Combine all ipd data
ipd <- adsl %>%
  full_join(adrs, by = c("USUBJID", "ARM")) %>%
  full_join(adtte, by = c("USUBJID", "ARM"))
head(ipd)
```

## Preprocessing aggregate data

There are two ways of specifying aggregate data. One approach is to read in aggregate data using an excel spreadsheet. In the spreadsheet, possible variable types include mean, median, or standard deviation for continuous variables and count or proportion for binary variables. The naming should be followed by these suffixes accordingly: _COUNT, _MEAN, _MEDIAN, _SD, _PROP. Then, `process_agd` will convert the count into proportions.

Other way is to define data frame of aggregate data in R. If you do it this way, _COUNT prefix should not be specified and only proportion is allowed for binary variables. Other suffix names would be the same as the first approach.

Possible missingness in the binary variables should be accounted for by subtracting the denominator by the missing count i.e. proportion = count / (N - missing).

```{r}
# Through excel spreadsheet
# target_pop <- read.csv(system.file("extdata","aggregate_data_example_1.csv", package = "maicplus", mustWork = TRUE))
# target_pop2 <- read.csv(system.file("extdata", "aggregate_data_example_2.csv", package = "maicplus", mustWork = TRUE))
# target_pop3 <- read.csv(system.file("extdata", "aggregate_data_example_3.csv", package = "maicplus", mustWork = TRUE))
# agd <- process_agd(target_pop)

# Second approach by defining a data frame in R
agd <- data.frame(
  STUDY = "Lung study",
  ARM = "Total",
  N = 300,
  AGE_MEAN = 51,
  AGE_MEDIAN = 49,
  AGE_SD = 3.25,
  SEX_MALE_PROP = 147/300,
  ECOG0_PROP = 0.40,
  ECOG0_COUNT = 40,
  SMOKE_PROP = 58/(300-5),
  N_PR_THER_MEDIAN = 2
)
```

## Preprocessing aggregate data

```{r}
#### prepare data
ipd_centered <- center_ipd(ipd = ipd, agd = agd)
head(ipd_centered)
```

### How to handle standard deviation aggregate summary

As described by Phillippo et al. 2016, balancing on both mean and standard deviation for continuous variables (where possible) may be considered in some cases. If a standard deviation is provided in the comparator population, preprocessing is done so that in the target population, $E(X^2)$ is calculated using the variance formula $Var(X)=E(X^{2})-E(X)^{2}$. This $E(X^2)$ in the target population is matched with the IPD level data, which is why $X^{2}$ was calculated during the preprocessing stage of IPD.

### How to handle median aggregate summary

If a median is provided, IPD is preprocessed to categorize the variable into a binary variable. All the values in the IPD that are higher than the comparator population median is assigned a value of 1. Conversely, all values that are lower are assigned a value of 0. Comparator population median is replaced by 0.5 to adjust to the categorization in the IPD data. The newly created IPD binary variable is matched so that the proportion is 0.5.

### How to handle missing counts in aggregate data

If there are missing counts in a categorical variable, we subtract the missing counts from the total sample size in the denominator and estimate the proportion with only the observed values. What are we assuming about the missing patients?

If $A$ is the number of event, $B$ is the number of non-event, and $N$ is the number of missing counts, we ignore $N$ and estimate the proportion of event by

\[
\frac{A}{A+B}
\]

If X is the number of missing event count, alternate way to estimate the proportion would be

\[
\frac{A+X}{A+B+N}
\]

We would like the two proportion to be the same. 

\[
\frac{A+X}{A+B+N}=\frac{A}{A+B}
\]

If we assume $X$ follows a binomial distribution with total size $N$ and probability of event $p$ and take the expectation on both sides we get

\[
\frac{A+Np}{A+B+N}=\frac{A}{A+B} 
\]

Solving for the equation

\begin{align*}
(A+Np)(A+B) & =A(A+B+N)\\
A^{2}+AB+ANp+BNp & =A^{2}+AB+AN\\
(A+B)Np & =AN\\
p & =\frac{A}{A+B}
\end{align*}

Thus, by subtracting missing counts $N$ from the total sample size in the denominator, we are assuming that the missing patients have probability of an event that is equal to the observed proportion of an event. This seems like a fair assumption to make if the patients are missing completely at random.

# Calculating weights

```{r}
# list variables that are going to be used to match
centered_colnames <- c("AGE", "AGE_SQUARED", "SEX_MALE", "ECOG0", "SMOKE", "N_PR_THER_MEDIAN")
centered_colnames <- paste0(centered_colnames, "_CENTERED")
  
match_res <- estimate_weights(data = ipd_centered, 
                              centered_colnames = centered_colnames)

# Alternatively, you can specify the numeric column locations for centered_colnames
# match_res <- estimate_weights(ipd_centered, centered_colnames = c(14, 16:20))

# Two options of plotting weights: unscaled or scaled
wt <- match_res$data$weights
wt_scaled <- match_res$data$scaled_weights

par(mfrow = c(1, 2))
plot_weights(wt, bin_col = "orange", vline_col = "darkblue")
plot_weights(wt_scaled, main_title = "Scaled Individual Weights")
```

There is also an option of printing weights using ggplot. 

```{r}
plot_weights2(match_res, print_caption = TRUE, caption_width = 80)
```

Another check after the weights are calculated is to look at how the weighted covariates match with the aggregate data summary.

```{r}
outdata <- check_weights(match_res, agd)
outdata
```


# Time to event analysis

We first need to combine internal IPD data with pseudo comparator IPD. To obtain pseudo comparator IPD, we would digitize Kaplan Meier curves from the comparator study.

```{r}
pseudo_ipd <- read.csv(system.file("extdata", "psuedo_IPD.csv", package = "maicplus", mustWork = TRUE))
pseudo_ipd$ARM <- "B" #Need to specify ARM for pseudo ipd
matched_ipd <- match_res$data

combined_data_tte <- merge_two_data(pseudo_ipd, matched_ipd, internal_time_name = "TIME", internal_event_name = "EVENT")
str(combined_data_tte)
```

## Report 1: Kaplan-Meier plot

```{r}
kmobj <- survfit(Surv(TIME, EVENT) ~ ARM, combined_data_tte, conf.type = "log-log")
kmobj_adj <- survfit(Surv(TIME, EVENT) ~ ARM, combined_data_tte, weights = combined_data_tte$weight, conf.type = "log-log")

par(cex.main=0.85)
km_plot(kmobj, kmobj_adj, time_scale = "month", trt = "A", trt_ext = "B", endpoint_name = "OS")
```

There is also a ggplot option for Kaplan-Meier curves using `survminer` R package.

```{r}
km_plot2(combined_data_tte, trt = "A", trt_ext = "B", censor = TRUE, risk.table = TRUE)
```

## Report 2: Analysis table (Cox model) before and after matching, incl Median Survival Time

We can then fit a cox regression model using the combined dataset. For the weight adjusted cox regression, we fit the model with robust standard errors.
Along with the hazard ratios, we can also find median survival time using `medSurv_makeup` function. Then, `report_table` function nicely combines the information together and create a result table.

```{r}
# Fit a Cox model with/without weights to estimate the HR
unweighted_cox <- coxph(Surv(TIME, EVENT==1) ~ ARM, data = combined_data_tte)
weighted_cox <- coxph(Surv(TIME, EVENT==1) ~ ARM, data = combined_data_tte, weights = combined_data_tte$weights, robust = TRUE)

# Derive median survival time
medSurv <- medSurv_makeup(kmobj, legend = "before matching", time_scale = "day")
medSurv_adj <- medSurv_makeup(kmobj_adj, legend = "after matching", time_scale = "day")
medSurv_out <- rbind(medSurv, medSurv_adj)
medSurv_out

rbind(
  report_table(unweighted_cox, medSurv, tag = paste0("Before/", "Overall survival")),
  report_table(weighted_cox, medSurv_adj, tag = paste0("After/", "Overall survival"))
)
```

## Report 3: Bootstrap result

```{r, results = 'hide'}
set.seed(1)
HR_bootstraps <- boot(data = matched_ipd,
                      statistic = bootstrap_HR, 
                      centered_colnames = centered_colnames, 
                      internal_time_name = "TIME", 
                      internal_event_name = "EVENT",
                      external = pseudo_ipd,
                      R = 1000)
```

```{r}
# Median of the bootstrap samples
HR_median <- median(HR_bootstraps$t)

# Bootstrap CI - Percentile CI
boot_ci_HR <- boot.ci(boot.out = HR_bootstraps, index=1, type="perc")

# Bootstrap CI - BCa CI
boot_ci_HR_BCA <- boot.ci(boot.out = HR_bootstraps, index=1, type="bca")

HR_median
boot_ci_HR
boot_ci_HR_BCA
```


## Report 4: Diagnosis Plot

```{r}
# grambsch & theaneu ph test
coxdiag <- cox.zph(unweighted_cox, global = F, transform = "log")
coxdiag_adj <- cox.zph(weighted_cox, global = F, transform = "log")

coxdiag
par(mfrow = c(1, 1), tcl = -0.15)
plot(coxdiag, yaxt = "n", main = "Grambsch & Terneau Plot (before matching)")
axis(2, las = 1)

coxdiag_adj
par(mfrow = c(1, 1), tcl = -0.15)
plot(coxdiag_adj, yaxt = "n", main = "Grambsch & Terneau Plot (after matching)")
axis(2, las = 1)

# log-cumulative hazard plot
log_cum_haz_plot(kmobj,
  time_scale = "month", log_time = TRUE,
  endpoint_name = "OS", subtitle = "(Before Matching)"
)

log_cum_haz_plot(kmobj_adj,
  time_scale = "month", log_time = TRUE,
  endpoint_name = "OS", subtitle = "(After Matching)"
)

# schoenfeld residual plot
resid_plot(unweighted_cox,
  time_scale = "month", log_time = TRUE,
  endpoint_name = "OS", subtitle = "(Before Matching)"
)

resid_plot(weighted_cox,
  time_scale = "month", log_time = TRUE,
  endpoint_name = "OS", subtitle = "(After Matching)"
)
```

## Analysis using a built-in wrapper

One can do all this analysis in a wrapper

```{r, results = 'hide', fig.show = 'hide'}
fit_unanchored <- maic_tte_unanchored(external = pseudo_ipd, internal = matched_ipd, internal_time_name = "TIME", internal_event_name = "EVENT", time_scale = "month", endpoint_name = "OS", transform = "log")
```

```{r}
names(fit_unanchored)
```

# Binary outcome analysis

## Analysis without wrapper

```{r, eval = FALSE}
# Simulate response data based on the known proportion of responders
comparator_prop_events <- 0.4

# Calculate number with event. Use round() to ensure we end up with a whole number of people. number without an event = Total N - number with event to ensure we keep the same number of patients
n_with_event <- round(agd$N*comparator_prop_events, digits = 0)
comparator_binary <- data.frame("RESPONSE"= c(rep(1, n_with_event), rep(0, agd$N - n_with_event)))
comparator_binary$ARM <- "B" # need to specify ARM for comparator data
matched_ipd <- match_res$data

combined_data_binary <- merge_two_data(comparator_binary, matched_ipd, internal_response_name = "RESPONSE")

unweighted_OR <- glm(formula = RESPONSE ~ ARM,
                     family = binomial(link="logit"),
                     data = combined_data_binary)

# Log odds ratio
log_OR_CI <- cbind(coef(unweighted_OR), confint.default(unweighted_OR, level = 0.95))[2,]

# Odds ratio
OR_CI <- exp(log_OR_CI)
names(OR_CI) <- c("OR", "OR_low_CI", "OR_upp_CI")
OR_CI

# Fit a logistic regression model with weights to estimate the weighted OR
weighted_OR <- suppressWarnings(glm(formula = RESPONSE ~ ARM,
                                    family = binomial(link="logit"),
                                    data = combined_data_binary,
                                    weight = weights))

# Weighted log odds ratio
log_OR_CI_wtd <- cbind(coef(weighted_OR), confint.default(weighted_OR, level = 0.95))[2,]

# Weighted odds ratio
OR_CI_wtd <- exp(log_OR_CI_wtd)
names(OR_CI_wtd) <- c("OR", "OR_low_CI", "OR_upp_CI")
OR_CI_wtd

# Robust standard error
vmod <- clubSandwich::vcovCR(weighted_OR, cluster= 1: dim(combined_data_binary)[1],type="CR2")
coef_res <- clubSandwich::conf_int(weighted_OR,vmod,coef=2)

OR_CI_robust <- exp(with(coef_res, c(beta, CI_L, CI_U)))
names(OR_CI_robust) <- c("Estimate", "Lower 95% CI", "Upper 95% CI")
OR_CI_robust

# Using sandwich package
V.sw <- sandwich::vcovHC(weighted_OR) #white's estimator
SD <- sqrt(V.sw[2,2])
Estimate <- coef(weighted_OR)[2]
OR_CI_robust2 <- exp(c(Estimate, Estimate - 1.96*SD, Estimate + 1.96*SD))
names(OR_CI_robust2) <- c("Estimate", "Lower 95% CI", "Upper 95% CI")
OR_CI_robust2
```

# Things on the to-do list

bootstrap, kaplan meier plots, anchored comparison example

```{r, eval = FALSE, echo = FALSE}
target_pop <- data.frame(
  STUDY = "Study_XXXX",
  ARM = "Total",
  N = 300,
  AGE_MEAN = 51,
  AGE_MEDIAN = 49,
  AGE_SD = 3.25,
  SEX_MALE_COUNT = 147,
  ECOG0_COUNT = 105,
  SMOKE_COUNT = 58/(300-5)
)

target_pop <- process_agd(target_pop)

# AC trial
AC_trial <- generate_survival_data(marginal_X_mean = 0)
AC_trial[,"x2"] <- ifelse(AC_trial[,"x2"] > 0, 1, 0)
AC_trial[,"x3"] <- ifelse(AC_trial[,"x3"] > 0.1, 1, 0)
AC_trial[,"x4"] <- ifelse(AC_trial[,"x4"] > 0.05, 1, 0)
colnames(AC_trial)[5:8] <- c("AGE", "SEX", "ECOG", "SMOKE")

A_trial <- AC_trial[AC_trial$treat == 1,]

BC_trial <- generate_survival_data(N = 600, marginal_X_mean = 0.45)
B_trial <- BC_trial[BC_trial$treat == 1,]
```


